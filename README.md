# Miniâ€‘etcd

A **Go**â€‘based, Raftâ€‘backed keyâ€‘value store that reâ€‘implements the **core of etcd**, focus on Raft consensus, WAL +
snapshot persistence, MVCC state machine, linearizable KV & Watch API.

---

## ğŸš¦Project Phases

| Phase                         | Target                                     | Deliverables                                         |
|-------------------------------|--------------------------------------------|------------------------------------------------------|
| **0. Local KV (DONE)**        | Fast inâ€‘memory Bâ€‘Tree with MVCC            | `PUT / GET / DELETE / RANGE / WATCH` over gRPC       |
| **1. Durability (DONE)**      | **WAL** + **snapshots** for crash recovery | Automatic replay at startup                          |
| **2. High Availability(WIP)** | **Single Raft Group** (3â€“5 nodes)          | Leader election, log replication, strong consistency |
| **3. Production Polish**      | CLI, Web UI                                | Opsâ€‘friendly experience                              |

> **Why this order?**Build a correct singleâ€‘node database â†’ make it durable â†’ replicate via Raft â†’ refine UX & security.

---

## ğŸ—ï¸ Architecture

```
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  RPC (HTTP/2)
 client â”€â”€â–¶â”‚  gRPC Client â”‚  PUT / GET / DELETE / RANGE / WATCH
           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ linearizableâ€‘read hits **Leader**
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         miniâ€‘etcd Node       â”‚   (all nodes run identical stack)
        â”‚                              â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚   Watch fanâ€‘out
        â”‚  â”‚   KvServer   â”‚â—„â”€â”€â”€â”       â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚       â”‚
        â”‚        â–¼  apply      â”‚       â”‚  revision â†’ event
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚       â”‚
        â”‚  â”‚   Bâ€‘Tree     â”‚    â”‚       â”‚  MVCC stateâ€‘machine
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚       â”‚
        â”‚        â–² snapshot    â”‚       â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚       â”‚
        â”‚  â”‚     WAL      â”‚â”€â”€â”€â”€â”˜       â”‚   append + fsync
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
        â”‚        â–² Raft log            â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
        â”‚  â”‚  Raft Node   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€ replicate entries to followers
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

* **Full replication, single Raft group.**  Each node maintains the **entire** keyâ€‘space; quorum replication guarantees
  strong consistency & faultâ€‘tolerance (2F+1 â†’ tolerate F failures).
* **Write path**: client â†’ Leader â†’ Raft **append** â†’ WAL **fsync** â†’ **commit** â†’ apply to Bâ€‘Tree â†’ notify watchers.
* **Read path**:
    * **Linearizable** (default): go to Leader (or use ReadIndex) so that `rev â‰¤ commitIndex`.
    * **Serializable** (optâ€‘in): issue to any followerâ€”may lag by a few entries.
* **Watch**: each event carries `modRevision == raftIndex`; consumers can resume seamlessly after reconnect.

---

## âœ¨ Current Feature Set (Phase 0 & Phase 1)

* **Google Bâ€‘Tree** (degree 64) â†’ *O(logn)* read/write
* **MVCC** with global `revision` & perâ€‘key versioning
* **gRPC v3â€‘subset** (`Put / Range / DeleteRange / Watch`)
* **Concurrency** via `sync.RWMutex` + goroutines; nonâ€‘blocking watch fanâ€‘out
* **Deterministic stateâ€‘machine** interface ready for Raft integration
* **Writeâ€‘Ahead Log (WAL)** â€“ protobufâ€‘framed, `fsync` on commit
* **Snapshots & Compaction** â€“ periodic full dump, WAL truncation, defrag API
* **Crash Recovery** â€“ snapshot load â†’ WAL replay â†’ serve

---

## ğŸ”œ Nearâ€‘Term TODO (Phase 2 & 3)

* **Singleâ€‘Group Raft** â€“ integrate `raft`; election, log replication, ReadIndex path
* **Health & Metrics** â€“ `/health`, Prometheus counters
* **Chaos Tests** â€“ killâ€‘9, network partition; target failover <2 s

---

## ğŸš€ Quick Start (TODO)

```bash
# Run singleâ€‘node dev server
mini-etcd --data=data/default --listen=:2379

# Put / Get via CLI (sample)
mini-etcdctl --endpoint localhost:2379 put foo bar
mini-etcdctl --endpoint localhost:2379 get foo
```

**3â€‘node cluster** once Raft is ready:

```bash
mini-etcd --name n1 --listen=:2380 --peers=n1:2380,n2:2381,n3:2382 &
mini-etcd --name n2 --listen=:2381 --peers=n1:2380,n2:2381,n3:2382 &
mini-etcd --name n3 --listen=:2382 --peers=n1:2380,n2:2381,n3:2382 &
```

---

## ğŸ› ï¸ Core Tech Stack

* **Go 1.22+** â€“ goroutines, generics, lowâ€‘level `sync` primitives
* **gRPC & Protocol Buffers** â€“ compact binary RPC over HTTP/2
* **Google Bâ€‘Tree** â€“ inâ€‘memory ordered storage engine
* **raft** â€“ consensus & replication layer
* **WAL + Snapshots** â€“ durability, crash recovery
* **Prometheus & OpenTelemetry** â€“ metrics and tracing
* **Docker** â€“ reproducible dev & CI environment

---

## â¤ï¸ Contributing

PRs & issues welcomeâ€”especially around Raft integration, WAL format, and snapshot tuning.

---

## License

MIT
